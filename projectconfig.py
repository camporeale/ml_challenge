# CPUs in system
n_cpus = 8

# Base directories - EDIT
base_data_dir = '../data/'
base_model_dir= '../models/'

# Original MercadoLibre files - EDIT
data_files = {
              'train': base_data_dir + 'train.csv', 
              'test':  base_data_dir + 'test.csv'
             }

# Files generated by preprocessing
normalized_files = {
                    'train': base_data_dir + 'train_full_norm.csv', 
                    'validation': base_data_dir + 'val_full_norm.csv', 
                    'test': base_data_dir + 'test_full_norm.csv'
                    }

normalized_language_files = {
                            'spanish':{'train': base_data_dir + 'train_spanish_norm.csv',
                                        'validation': base_data_dir + 'val_spanish_norm.csv',
                                        'test': base_data_dir + 'test_spanish_norm.csv'}, 
                             'portuguese': {'train': base_data_dir + 'train_portuguese_norm.csv',
                                            'validation': base_data_dir + 'val_portuguese_norm.csv',
                                            'test': base_data_dir + 'test_portuguese_norm.csv'},
                             'mapping':{'test': base_data_dir + 'language_mapping_test.csv'}
                            }                                       

normalized_reliable_files = {
                            'train': base_data_dir + '/train_reliable_norm.csv',
                            'validation': base_data_dir + '/val_reliable_norm.csv'
                            }

# Model files
models = {
          #"model_full_2gram": {"file": normalized_files["train"], "epoch": 5, "lr":0.5, "wordNgrams":2, "dim":200},
          #"model_full_3gram": {"file": normalized_files["train"], "epoch": 5, "lr":0.5, "wordNgrams":3, "dim":200},
          #"model_reliable_2gram": {"file": normalized_reliable_files["train"], "epoch": 5, "lr":0.5, "wordNgrams":2, "dim":200 },
          "model_reliable_3gram": {"file": normalized_reliable_files["train"], "epoch": 5, "lr":0.5, "wordNgrams":3, "dim":200},
          #"model_spanish_2gram": {"file": normalized_language_files["spanish"]["train"], "epoch": 5, "lr":0.5, "wordNgrams":2, "dim":200 },
          #"model_portuguese_2gram": {"file": normalized_language_files["portuguese"]["train"], "epoch": 5, "lr":0.5, "wordNgrams":2, "dim":200 }
          "model_spanish_3gram": {"file": normalized_language_files["spanish"]["train"], "epoch": 5, "lr":0.5, "wordNgrams":3, "dim":200 },
          "model_portuguese_3gram": {"file": normalized_language_files["portuguese"]["train"], "epoch": 5, "lr":0.5, "wordNgrams":3, "dim":200 }
        }

model_files = {
               "model_full_2gram":base_model_dir + "model_full_2gram",
               "model_full_3gram":base_model_dir + "model_full_3gram",
               "model_reliable_2gram":base_model_dir + "model_reliable_2gram",
              # "model_reliable_3gram":base_model_dir + "model_reliable_3gram", 
               "model_spanish_2gram":base_model_dir + "model_spanish_2gram", 
               #"model_spanish_3gram":base_model_dir + "model_spanish_3gram", 
               "model_portuguese_2gram":base_model_dir + "model_portuguese_2gram"
              # , "model_portuguese_3gram":base_model_dir + "model_portuguese_3gram"
              }

models_for_predict = {
               "model_full_2gram":model_files["model_full_2gram"],
               "model_full_3gram":model_files["model_full_3gram"],
               "model_reliable_2gram":model_files["model_reliable_2gram"],
               "model_bilingual": {"spanish":model_files["model_spanish_2gram"], 
                                   "portuguese":model_files["model_portuguese_2gram"]}
              }


# Submission File
submission_file = 'submission.csv'

